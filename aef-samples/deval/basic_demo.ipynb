{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: This is a sample notebook that gives hands on experience to some of the features of DEP AI deval package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: deval package and it's purpose\n",
    "Evaluating Large Language Models (LLMs) is crucial for optimizing their performance and ensuring their reliability.\n",
    "This evaluation typically occurs in two primary settings.\n",
    "The first is data-driven experimentation, where a set of data points is used to optimize solution parameters, prompts, and other variables.\n",
    "This is usually a repeatable, batch workflow where a large dataset is processed, and aggregated results are expected after a certain period.\n",
    "The second setting is an online environment, where results are instantly available for the current state of the solution and a small data sample.\n",
    "This online setting is useful for early experimentation when a larger dataset is not yet available or for online monitoring of a production solution to detect drift or other real-time metrics.\n",
    "\n",
    "The focus of the deval package is to provide quality evaluation, assessing the quality of a model's response based on user input and additional context.\n",
    "Since model responses are unstructured text, models themselves must evaluate the output, as there is no single correct answer and quality must be carefully defined.\n",
    "The models that assess the output of other LLMs are known as LLM-as-a-Judge.\n",
    "These are usually general-purpose models used in specific workflows to generate evaluation metrics.\n",
    "These workflows are not instantaneous, and using the models can be costly on a large scale.\n",
    "Therefore, LLMs-as-a-Judge are typically employed for batch processing to optimize solution parameters and detect drift, as well as for early online experimentation.\n",
    "\n",
    "Online monitoring of a production solution is generally expensive and only used for special applications.\n",
    "In such cases, other inexpensive, classic metrics like user-input length, model output length, latency, compute usage, etc., are more relevant.\n",
    "\n",
    "Below we will show at first the experimentation against a dataset to test and optimize a solution.\n",
    "Subsequently, the deval with online monitoring is conducted where an immediate response is available.\n",
    "Online monitoring is also instrumental in identifying LLM-as-a-Judge metrics and customizing them to meet project needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "In this Codespace has the DEval python SDK is already installed from the Deloitte artifactory. Four major ways exist to install the DEval sdk:\n",
    "- Through Deloitte Artifactory\n",
    "- Download the latest release as zip: https://github.com/Deloitte-US-Engineering/devals/releases\n",
    "- Clone the source code repo: https://github.com/Deloitte-US-Engineering/devals\n",
    "- Build a wheel file from the source code (get release code or clone repo and use a tool like : https://pypi.org/project/setuptools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an LLM as a Judge and embedding model\n",
    "\n",
    "TL;DR\n",
    "\n",
    "To effectively evaluate your GenAI solution, an LLM-as-a-Judge is utilized to score unstructured data. This process involves using a general-purpose LLM to generate a score, where a higher score indicates better quality.\n",
    "\n",
    "We employ Claude-3-5-Haiku, a general-purpose LLM, to serve as the judge for our data. This LLM is responsible for creating scores that assess the quality of the data, with higher scores representing better outcomes. The Claude-3-5-Haiku instance is managed by LiteLLM, which is integrated into the DEP AI LLM Gateway accelerator. LiteLLM operates within a container in this Codespace and can be accessed locally via port 4000. The LLM-as-a-Judge is crucial for scoring the unstructured data processed by a GenAI solution. Evaluation frameworks have developed specialized workflows to generate scores based on specific metrics, ensuring a comprehensive assessment of data quality.\n",
    "\n",
    "Some metrics use the semantic distance, for this we need an embedding model. In this example we use a self-hosted embedding model managed by Ollama and managed by LiteLLM inside the LLM Gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "my_llm = ChatOpenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=\"sk-TE5BPNfSh4IOCNpW3I5EDQ\",  # if you prefer to pass api key in directly instead of using env vars\n",
    "    base_url=\"http://0.0.0.0:4000\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=\"sk-TE5BPNfSh4IOCNpW3I5EDQ\",\n",
    "    base_url=\"http://0.0.0.0:4000\",\n",
    "    model=\"gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the evalution data\n",
    "\n",
    "TL;DR\n",
    "- deval provides a loose data structure defining the an atomic request\n",
    "\n",
    "Data for evaluating an LLM can always be abstracted to a question, an answer, a context, and a ground truth. The context can be many things, a context from a RAG pipeline, search engine results, chat history, or a full-text. The ground truth is also not a strict structure, it can be an entity for classification, a json for extraction tasks, or a set of possible answers.\n",
    "\n",
    "In devals we use the TypedDict EvaluatorRequest defining the question, answer, context, and ground truth structure for one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deval.evaluators.models import EvaluatorRequest\n",
    "#### The EvaluatorRequest is our standard dataformat, it is a TypedDict so it is not checked during runtime for correctness ####\n",
    "#help(EvaluatorRequest) # Execute me for more information\n",
    "\n",
    "my_evaluation_data: EvaluatorRequest = {\n",
    "    \"question\": \"How is the weather today\",\n",
    "    \"answer\": \"Raining cats and dogs\",\n",
    "    \"context\": \"Currently a storm system is present rotating around an area of low pressure, which produces strong winds and heavy rain.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pre-defined evaluation workflow\n",
    "\n",
    "TL;DR\n",
    "- In deval, a “preset” is a predefined workflow for evaluating LLM outputs.\n",
    "- Presets combine a JSON-like structure for metrics and options, and can include custom Python code.\n",
    "- Presets are portable and can be shared or executed anywhere.\n",
    "\n",
    "In deval, a preset is a predefined workflow used to evaluate LLM outputs. Each preset consists of two main components:\n",
    "\n",
    "1. A JSON-like structure that defines the metrics to be used and basic customization options.\n",
    "2. Optional Python code for custom instructions to be executed at runtime.\n",
    "\n",
    "A preset contains all the necessary information to evaluate an LLM output, making it easy to create once and reuse anywhere, at any time. Users are free to share presets in any format they choose; deval does not enforce a specific sharing method.\n",
    "\n",
    "For example, when evaluating question-answer scenarios, we use the “question answer” preset, which includes four metrics: groundedness, response_drift_embedding, response_drift, and coherence_measure. In our minimal example, the response drift metrics cannot be calculated because they require ground truth data, which is not present in our input. These metrics will be automatically skipped during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from deval.presets.qa_preset import QAPreset\n",
    "#### The QAPreset is a pre-defined workflow ####\n",
    "#help(QAPreset()) # Execute me for more information\n",
    "\n",
    "evaluation_workflow = QAPreset()\n",
    "evaluation_workflow.set_llm_as_a_judge(my_llm)\n",
    "evaluation_workflow.set_embedding_model(embeddings)\n",
    "\n",
    "#### You can get the metrics inside the preset with get_metrics() ####\n",
    "print(\"These are the names of the metrics inside the workflow.\\n\" \\\n",
    "\"We have standardized the names of the metrics based on this work:https://kx.deloitte/documents/view/92541 \\n\" \\\n",
    "\"Nevertheless, the native names of the evaluation frameworks are also possible to let users decide which naming convention they prefer.\\n\")\n",
    "pprint.pprint([\"Framework Name(s): {} \\nDeloitte Name: {} \\n {}\".format(x['name'], x['name_deloitte'], \"-\"*10) for x in evaluation_workflow.get_metrics()])\n",
    "\n",
    "# pprint.pprint([x for x in evaluation_workflow.get_metrics()]) # Execute me for more detailed information about the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "TL;DR\n",
    "- If the data does not meet a metric’s requirements within a preset, that metric will be skipped.\n",
    "- Langfuse integration is enabled by default in Deval but can be explicitly disabled.\n",
    "- Deval returns results in a standardized format, consistent across different evaluation frameworks.\n",
    "\n",
    "All Deval metrics include metadata that indicates whether a metric can be calculated. If a metric cannot be calculated, usually due to insufficient data, it will be skipped and a warning will be logged.\n",
    "\n",
    "For monitoring, Deval is automatically integrated with Langfuse. This integration can be disabled by setting the environment variable DEVAL_LANGFUSE to \"0\" or \"False\". Langfuse integration will also be deactivated if the Python Langfuse SDK is not installed or if the Langfuse API key is missing from the environment variables.\n",
    "\n",
    "Deval outputs results as a standardized dictionary, ensuring consistency across all supported evaluation frameworks. Additionally, Deval can automatically track results in monitoring tools like Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Explicitly deactivate langfuse inside devals to keep it minimal ####\n",
    "import os\n",
    "os.environ['DEVAL_LANGFUSE'] = \"0\"\n",
    "result = evaluation_workflow.calc_metrics(my_evaluation_data)\n",
    "\n",
    "#### The return is a list of dictionaries for each metric, the metrics 'response_drift_embedding' and  'response_drift' require a ground truth, which was not given, so they are in the results  ####\n",
    "print([\"{} : {}\".format(x[\"name\"], x[\"score\"]) for x in result])\n",
    "\n",
    "#### Warning ####\n",
    "# It can happen that some metrics need to be skipped since the LLM is not responding as the evaluation framework expected it.\n",
    "# This can have multiple reasons but usually the prompts inside the evaluation frameworks were designed for one LLM type and version.\n",
    "# This underlines the importance of defining strict presets including LLM definition and settings to ensure reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples, see https://github.com/Deloitte-US-Engineering/dep-devals/tree/main/deval/deval/examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
